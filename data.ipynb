{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332dcd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontradas 2 carpetas de chips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando carpetas: 100%|██████████| 2/2 [00:16<00:00,  8.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://source.coop/radiantearth/cloud-cover-detection-challenge/final/public/train_labels\"\n",
    "OUTPUT_DIR = \"train_labels_downloaded\"\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def list_links(url):\n",
    "    \"\"\"Devuelve todos los enlaces de una página.\"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "    return links\n",
    "\n",
    "# 1. Listar carpetas dentro de train_features\n",
    "folders = [f for f in list_links(BASE_URL) if f.endswith(\"/\")]\n",
    "\n",
    "print(f\"Encontradas {len(folders)} carpetas de chips.\")\n",
    "\n",
    "for folder in tqdm(folders, desc=\"Descargando carpetas\"):\n",
    "    folder_url = f\"{BASE_URL}/{folder}\"\n",
    "    folder_path = os.path.join(OUTPUT_DIR, folder.replace(\"/\", \"\"))\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # 2. Listar archivos dentro de cada carpeta\n",
    "    files = [f for f in list_links(folder_url) if f.endswith(\".tif\")]\n",
    "\n",
    "    for file in files:\n",
    "        file_url = f\"{folder_url}/{file}\"\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        # 3. Descargar archivo\n",
    "        with requests.get(file_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe80676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La API no devolvió JSON válido o formato inesperado: Expecting value: line 1 column 1 (char 0)\n",
      "Status code: 200\n",
      "Content-Type: text/html; charset=utf-8\n",
      "Primeros 500 caracteres de la respuesta: <!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"stylesheet\" href=\"/_next/static/css/1e5913857760d7aa.css?dpl=dpl_8jQWPjTKwo8ZPwQi3Q1LDvYUm6dy\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/fdbd78beb3744130.css?dpl=dpl_8jQWPjTKwo8ZPwQi3Q1LDvYUm6dy\" data-precedence=\"next\"/><link rel=\"stylesheet\" href=\"/_next/static/css/7abd75a0e14f04fb.css?dpl=dpl_8jQWPjTKwo8ZPwQi3Q1LDvYUm6dy\" data-\n",
      "Error al acceder a la API: Expecting value: line 1 column 1 (char 0)\n",
      "Usando método alternativo: scraping HTML...\n",
      "Encontradas 2 carpetas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Listando archivos: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrados 0 archivos .tif mediante scraping HTML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando archivos: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_URL = \"https://source.coop/api/v1/radiantearth/cloud-cover-detection-challenge/final/public/train_features\"\n",
    "BASE_URL = \"https://source.coop/radiantearth/cloud-cover-detection-challenge/final/public/train_features\"\n",
    "OUT = \"train_features_full\"\n",
    "\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "def list_links(url):\n",
    "    \"\"\"Devuelve todos los enlaces de una página.\"\"\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "    return links\n",
    "\n",
    "# 1. Intentar obtener lista desde la API, si falla usar scraping HTML\n",
    "try:\n",
    "    resp = requests.get(API_URL)\n",
    "    resp.raise_for_status()\n",
    "    \n",
    "    # Verificar si la respuesta es JSON válido\n",
    "    try:\n",
    "        data = resp.json()\n",
    "        # Si es una lista de objetos con \"path\" y \"type\"\n",
    "        if isinstance(data, list) and len(data) > 0 and \"path\" in data[0]:\n",
    "            files = [f for f in data if f[\"type\"] == \"file\" and f[\"path\"].endswith(\".tif\")]\n",
    "            print(f\"Encontrados {len(files)} archivos .tif desde la API\")\n",
    "        else:\n",
    "            raise ValueError(\"Formato de API no esperado\")\n",
    "    except ValueError as e:\n",
    "        print(f\"La API no devolvió JSON válido o formato inesperado: {e}\")\n",
    "        print(f\"Status code: {resp.status_code}\")\n",
    "        print(f\"Content-Type: {resp.headers.get('Content-Type', 'N/A')}\")\n",
    "        print(f\"Primeros 500 caracteres de la respuesta: {resp.text[:500]}\")\n",
    "        raise\n",
    "except (requests.exceptions.RequestException, ValueError) as e:\n",
    "    print(f\"Error al acceder a la API: {e}\")\n",
    "    print(\"Usando método alternativo: scraping HTML...\")\n",
    "    \n",
    "    # Método alternativo: usar scraping HTML como en la celda anterior\n",
    "    folders = [f for f in list_links(BASE_URL) if f.endswith(\"/\")]\n",
    "    print(f\"Encontradas {len(folders)} carpetas.\")\n",
    "    \n",
    "    files = []\n",
    "    for folder in tqdm(folders, desc=\"Listando archivos\"):\n",
    "        folder_url = f\"{BASE_URL}/{folder}\"\n",
    "        folder_files = [f for f in list_links(folder_url) if f.endswith(\".tif\")]\n",
    "        for file in folder_files:\n",
    "            files.append({\n",
    "                \"path\": f\"{folder}{file}\",\n",
    "                \"type\": \"file\"\n",
    "            })\n",
    "    \n",
    "    print(f\"Encontrados {len(files)} archivos .tif mediante scraping HTML\")\n",
    "\n",
    "# 2. Descargar cada archivo\n",
    "for f in tqdm(files, desc=\"Descargando archivos\"):\n",
    "    if isinstance(f, dict):\n",
    "        file_path_rel = f[\"path\"]\n",
    "    else:\n",
    "        file_path_rel = f\n",
    "    \n",
    "    file_url = f\"{BASE_URL}/{file_path_rel}\"\n",
    "    local_path = os.path.join(OUT, file_path_rel)\n",
    "\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(local_path):\n",
    "        r = requests.get(file_url, stream=True)\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, \"wb\") as fp:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                fp.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://source.coop/radiantearth/cloud-cover-detection-challenge/final/public\"\n",
    "OUTPUT_DIR = \"descargas\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; YourBot/0.1; +https://example.com/bot)\"\n",
    "})\n",
    "\n",
    "def get_soup(url, retries=3, backoff=2):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = session.get(url, timeout=20)\n",
    "            r.raise_for_status()\n",
    "            return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(backoff * (i + 1))\n",
    "\n",
    "def is_same_domain(url, base_domain):\n",
    "    parsed = urlparse.urlparse(url)\n",
    "    return parsed.netloc == \"\" or parsed.netloc == base_domain\n",
    "\n",
    "def download_file(url, root_url, output_dir):\n",
    "    rel_path = urlparse.urlparse(url).path.replace(urlparse.urlparse(root_url).path, \"\", 1)\n",
    "    rel_path = rel_path.lstrip(\"/\")\n",
    "    if not rel_path:\n",
    "        return\n",
    "\n",
    "    local_path = os.path.join(output_dir, rel_path)\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(local_path):\n",
    "        return\n",
    "\n",
    "    with session.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "def crawl(url, root_url, output_dir, visited, file_exts=(\".tif\", \".tiff\")):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    soup = get_soup(url)\n",
    "    base_domain = urlparse.urlparse(root_url).netloc\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        full = urlparse.urljoin(url, href)\n",
    "\n",
    "        if not is_same_domain(full, base_domain):\n",
    "            continue\n",
    "\n",
    "        path = urlparse.urlparse(full).path\n",
    "\n",
    "        if any(path.lower().endswith(ext) for ext in file_exts):\n",
    "            download_file(full, root_url, output_dir)\n",
    "        else:\n",
    "            # Heurística simple de “directorio”: termina en “/”\n",
    "            if path.endswith(\"/\"):\n",
    "                crawl(full, root_url, output_dir, visited, file_exts=file_exts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visited = set()\n",
    "    crawl(BASE_URL, BASE_URL, OUTPUT_DIR, visited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d137933c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchBucket",
     "evalue": "An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¡Descarga completada!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mdownload_s3_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 27\u001b[0m, in \u001b[0;36mdownload_s3_folder\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m pages \u001b[38;5;241m=\u001b[39m paginator\u001b[38;5;241m.\u001b[39mpaginate(Bucket\u001b[38;5;241m=\u001b[39mBUCKET, Prefix\u001b[38;5;241m=\u001b[39mPREFIX)\n\u001b[0;32m     26\u001b[0m tif_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpages\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mContents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\paginate.py:272\u001b[0m, in \u001b[0;36mPageIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inject_starting_params(current_kwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_parsed_response(response)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_request:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# The first request is handled differently.  We could\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# possibly have a resume/starting token that tells us where\u001b[39;00m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;66;03m# to index into the retrieved page.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[0;32m    122\u001b[0m     hook()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\paginate.py:360\u001b[0m, in \u001b[0;36mPageIterator._make_request\u001b[1;34m(self, current_kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;129m@with_current_context\u001b[39m(partial(register_feature_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAGINATOR\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_kwargs):\n\u001b[1;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcurrent_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[0;32m    122\u001b[0m     hook()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\botocore\\client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m request_context\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code_override\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1076\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "OUTPUT_DIR = \"train_features_downloaded\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cliente S3 público (no necesita credenciales)\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    config=Config(signature_version=UNSIGNED),\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "BUCKET = \"aws-opendata-us-west-2\"\n",
    "PREFIX = \"ref_cloud_cover_detection_challenge_v1/final/public/train_features/\"\n",
    "\n",
    "def download_s3_folder():\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    \n",
    "    # Lista todos los objetos\n",
    "    pages = paginator.paginate(Bucket=BUCKET, Prefix=PREFIX)\n",
    "    \n",
    "    tif_files = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.tif'):\n",
    "                    tif_files.append(key)\n",
    "    \n",
    "    print(f\"Encontrados {len(tif_files)} archivos .tif\")\n",
    "    \n",
    "    # Descarga cada archivo\n",
    "    for key in tqdm(tif_files, desc=\"Descargando\"):\n",
    "        local_path = os.path.join(OUTPUT_DIR, os.path.relpath(key, PREFIX))\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(local_path):\n",
    "            continue\n",
    "            \n",
    "        s3.download_file(BUCKET, key, local_path)\n",
    "    \n",
    "    print(\"¡Descarga completada!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_s3_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6ea5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al llamar a S3:\n",
      "NoSuchBucket('An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist')\n",
      "Detalle de error S3: {'Code': 'NoSuchBucket', 'Message': 'The specified bucket does not exist', 'BucketName': 'aws-opendata-us-west-2'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "BUCKET = \"aws-opendata-us-west-2\"\n",
    "PREFIX = \"ref_cloud_cover_detection_challenge_v1/final/public/train_features/\"\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    config=Config(signature_version=UNSIGNED),\n",
    "    region_name=\"us-west-2\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=PREFIX, MaxKeys=1)\n",
    "    print(\"OK, respuesta S3:\")\n",
    "    print(resp.get(\"KeyCount\"), \"objetos encontrados (primer page)\")\n",
    "except Exception as e:\n",
    "    # Mostrar información detallada del error\n",
    "    print(\"Error al llamar a S3:\")\n",
    "    print(repr(e))\n",
    "    if hasattr(e, \"response\"):\n",
    "        print(\"Detalle de error S3:\", e.response.get(\"Error\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
