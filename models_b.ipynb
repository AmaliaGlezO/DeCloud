{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ecbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargar el JSON etiquetado (el archivo que subiste)\n",
    "with open('training_data.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)['TRAINING_DATA']\n",
    "\n",
    "# 1. Definir el mapeo de etiquetas\n",
    "# Todas las etiquetas de tu JSON. 'O' significa 'Outside' (Fuera de entidad).\n",
    "LABEL_NAMES = [\"O\", \"INGREDIENTE\", \"CANTIDAD\", \"UNIDAD\", \"UTENSILIO\", \"ACCION\", \"TIEMPO\",\"PREPARACION\",\n",
    "    \"INSTRUCCION\", \"COLOR\",\"ESTADO\", \"FORMA\", \"MEDIDA\", \"SUPERFICIE\", \"TECNICA\", \"TEMP\", \"TEXTURA\"\n",
    "]\n",
    "\n",
    "# Crear mapeo B- y I- (Beginning e Inside)\n",
    "tag_list = ['O']\n",
    "for label in LABEL_NAMES[1:]:\n",
    "    tag_list.append(f'B-{label}')\n",
    "    tag_list.append(f'I-{label}')\n",
    "tag_to_id = {tag: i for i, tag in enumerate(tag_list)}\n",
    "id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
    "\n",
    "def convert_json_to_iob(example, tokenizer):\n",
    "    \"\"\"Convierte un ejemplo de tu formato JSON a tokens y etiquetas IOB.\"\"\"\n",
    "    text = example['text']\n",
    "    entities = example['entities']\n",
    "\n",
    "    # Tokenización\n",
    "    tokens = tokenizer(text, return_offsets_mapping=True, truncation=False, add_special_tokens=True)\n",
    "    offsets = tokens['offset_mapping']\n",
    "\n",
    "    # Inicializar todas las etiquetas como 'O'\n",
    "    labels = [tag_to_id['O']] * len(offsets)\n",
    "\n",
    "    # Crear un mapa de caracteres a etiquetas\n",
    "    # Inicializar todos los caracteres como 'O'\n",
    "    char_labels = ['O'] * len(text)\n",
    "\n",
    "    # Marcar los rangos de caracteres con sus etiquetas\n",
    "    for entity in entities:\n",
    "        start_char, end_char = entity['start'], entity['end']\n",
    "        label = entity['label'].upper()\n",
    "\n",
    "        if f'B-{label}' not in tag_to_id:\n",
    "            print(f\"Advertencia: Etiqueta '{label}' no está en LABEL_NAMES. Saltando...\")\n",
    "            continue\n",
    "\n",
    "        # Marcar el primer carácter como B-, el resto como I-\n",
    "        if start_char < len(char_labels):\n",
    "            char_labels[start_char] = f'B-{label}'\n",
    "        for i in range(start_char + 1, min(end_char, len(char_labels))):\n",
    "            char_labels[i] = f'I-{label}'\n",
    "\n",
    "    # Asignar etiquetas a tokens basándose en su offset\n",
    "    for i, (offset_start, offset_end) in enumerate(offsets):\n",
    "        if offset_start is None or offset_end is None:\n",
    "            continue\n",
    "\n",
    "        # Verificar qué etiqueta tiene la mayoría de los caracteres en este token\n",
    "        if offset_start < offset_end:  # Token no especial\n",
    "            token_chars = char_labels[offset_start:offset_end]\n",
    "\n",
    "            # Contar las etiquetas en los caracteres de este token\n",
    "            from collections import Counter\n",
    "            counter = Counter(token_chars)\n",
    "\n",
    "            if counter:\n",
    "                # Eliminar 'O' del conteo para priorizar entidades\n",
    "                if 'O' in counter:\n",
    "                    del counter['O']\n",
    "\n",
    "                if counter:\n",
    "                    # Tomar la etiqueta más común\n",
    "                    most_common_label = counter.most_common(1)[0][0]\n",
    "\n",
    "                    # Si es una etiqueta B- o I-\n",
    "                    if most_common_label != 'O':\n",
    "                        # Determinar si este token debería ser B- o I-\n",
    "                        # Si el primer carácter del token es B-, entonces este token es B-\n",
    "                        if char_labels[offset_start].startswith('B-'):\n",
    "                            labels[i] = tag_to_id[char_labels[offset_start]]\n",
    "                        else:\n",
    "                            # Buscar si hay alguna etiqueta B- en este token\n",
    "                            has_b_label = any(label.startswith('B-') for label in token_chars if label != 'O')\n",
    "                            if has_b_label:\n",
    "                                # Encontrar la primera etiqueta B-\n",
    "                                for label in token_chars:\n",
    "                                    if label.startswith('B-'):\n",
    "                                        labels[i] = tag_to_id[label]\n",
    "                                        break\n",
    "                            else:\n",
    "                                # Usar la etiqueta más común\n",
    "                                labels[i] = tag_to_id[most_common_label]\n",
    "\n",
    "    return {\n",
    "        'tokens': tokens.tokens(),\n",
    "        'input_ids': tokens['input_ids'],\n",
    "        'attention_mask': tokens['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Seleccionar un modelo Transformer multilingüe (ideal para el español)\n",
    "MODEL_CHECKPOINT = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Aplicar la conversión a todos los datos de entrenamiento\n",
    "training_examples = [convert_json_to_iob(example, tokenizer) for example in data]\n",
    "# Filtrar ejemplos que podrían ser problemáticos o que solo contienen tokens especiales\n",
    "training_examples = [ex for ex in training_examples if any(l != tag_to_id['O'] for l in ex['labels'])]\n",
    "\n",
    "print(f\"Número total de etiquetas únicas IOB: {len(tag_list)}\")\n",
    "print(f\"Ejemplos de entrenamiento generados: {len(training_examples)}\")\n",
    "\n",
    "# Ejemplo de un dato procesado\n",
    "print(\"\\n--- Ejemplo de Dato Procesado ---\")\n",
    "print(\"Tokens:\", training_examples[0]['tokens'][:10])\n",
    "print(\"Etiquetas (IDs):\", training_examples[0]['labels'][:10])\n",
    "print(\"Etiquetas (IOB):\", [id_to_tag[l] for l in training_examples[0]['labels'][:10]])\n",
    "\n",
    "# Verificar la corrección\n",
    "test_example = data[0]\n",
    "converted = convert_json_to_iob(test_example, tokenizer)\n",
    "\n",
    "print(\"\\n=== VERIFICACIÓN DE CORRECCIÓN ===\")\n",
    "print(\"Texto:\", test_example['text'][:50], \"...\")\n",
    "print(\"\\nTokens y etiquetas:\")\n",
    "for i, (token, label_id) in enumerate(zip(converted['tokens'][:20], converted['labels'][:20])):\n",
    "    print(f\"{i:2d}. {token:15s} -> {id_to_tag[label_id]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
