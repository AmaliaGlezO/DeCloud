{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97497e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, size=(256, 256)):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.size = size \n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.tif'))])\n",
    "        self.mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith(('.tif'))])\n",
    "\n",
    "        # Transformaciones fijas para imagen (incluye normalización)\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Transformaciones fijas para máscara (SIN normalización, solo resize y tensor)\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: (x > 0.5).float())\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        return self.img_transform(image), self.mask_transform(mask)\n",
    "\n",
    "# Ahora creas el dataset así:\n",
    "train_dataset = CloudDataset(images_dir=\"overall-mask\", masks_dir=\"masked\", size=(256, 256))\n",
    "\n",
    "# Definir transformaciones para las imágenes de entrenamiento\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Redimensionar si es necesario\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalización estándar\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Crear el DataLoader para batch training\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2  # Número de procesos para carga paralela\n",
    ")\n",
    "\n",
    "# Probar que funciona\n",
    "print(f\"Número de imágenes en el dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Ver un batch de datos\n",
    "for images, masks in train_loader:\n",
    "    print(f\"Tamaño del batch de imágenes: {images.shape}\")\n",
    "    print(f\"Tamaño del batch de máscaras: {masks.shape}\")\n",
    "    print(f\"Rango de valores en imágenes: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "    print(f\"Valores únicos en máscaras: {torch.unique(masks)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class CloudAttentionUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # --- ENCODER (ResNet18) ---\n",
    "        base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.enc0 = nn.Sequential(*list(base.children())[:3]) # [64, 128, 128]\n",
    "        self.enc1 = base.layer1 # [64, 128, 128]\n",
    "        self.enc2 = base.layer2 # [128, 64, 64]\n",
    "        self.enc3 = base.layer3 # [256, 32, 32]\n",
    "        self.enc4 = base.layer4 # [512, 16, 16] (Bottleneck)\n",
    "\n",
    "        # --- DECODER + ATTENTION ---\n",
    "        # Up 1: 16x16 -> 32x32\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.att3 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.conv3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Up 2: 32x32 -> 64x64\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.att2 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Up 3: 64x64 -> 128x128\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.att1 = AttentionBlock(F_g=64, F_l=64, F_int=32)\n",
    "        self.conv1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Capa Final de Salida (256x256)\n",
    "        self.final_up = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.enc0(x)\n",
    "        x1 = self.enc1(x0)\n",
    "        x2 = self.enc2(x1)\n",
    "        x3 = self.enc3(x2)\n",
    "        x4 = self.enc4(x3) # Bottleneck\n",
    "\n",
    "        # Decoder con Skip Connections y Atención\n",
    "        d3 = self.up3(x4)\n",
    "        s3 = self.att3(g=d3, x=x3)\n",
    "        d3 = torch.cat((s3, d3), dim=1) # Concatenación (512 canales)\n",
    "        d3 = F.relu(self.conv3(d3))\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        s2 = self.att2(g=d2, x=x2)\n",
    "        d2 = torch.cat((s2, d2), dim=1) # Concatenación (256 canales)\n",
    "        d2 = F.relu(self.conv2(d2))\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        s1 = self.att1(g=d1, x=x1)\n",
    "        d1 = torch.cat((s1, d1), dim=1) # Concatenación (128 canales)\n",
    "        d1 = F.relu(self.conv1(d1))\n",
    "\n",
    "        # Salida final escalada al tamaño original\n",
    "        out = self.final_up(d1)\n",
    "        out = self.final_conv(out)\n",
    "\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87416f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        # Binary Cross Entropy\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "\n",
    "        # Dice Loss\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return BCE + dice_loss\n",
    "\n",
    "# Instanciar modelo, loss y optimizador\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CloudAttentionUNet().to(device)\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Época [{epoch+1}/{epochs}], Pérdida: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # GUARDAR EL MODELO\n",
    "    torch.save(model, 'cloud_segmentation_model_2.pth')\n",
    "    print(\"Modelo guardado como 'cloud_segmentation_model.pth'\")\n",
    "\n",
    "# Ejecutar entrenamiento\n",
    "train_model(epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
