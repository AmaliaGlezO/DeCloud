{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad602aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "landsat-c2-l2 Landsat Collection 2 Level-2\n",
      "sentinel-2-l2a Sentinel-2 Level-2A\n",
      "landsat-c2-l1 Landsat Collection 2 Level-1\n",
      "hls2-s30 Harmonized Landsat Sentinel-2 (HLS) Version 2.0, Sentinel-2 Data\n",
      "hls2-l30 Harmonized Landsat Sentinel-2 (HLS) Version 2.0, Landsat Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amalia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pystac_client\\item_search.py:925: FutureWarning: get_items() is deprecated, use items() instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "\n",
    "catalog = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")\n",
    "\n",
    "min_lon, min_lat, max_lon, max_lat = -123.5, 37.0, -121.5, 38.5\n",
    "# Buscar colecciones relevantes por texto\n",
    "for col in catalog.get_all_collections():\n",
    "    if \"Sentinel-2\" in col.title or \"Landsat\" in col.title:\n",
    "        print(col.id, col.title)\n",
    "\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=[min_lon, min_lat, max_lon, max_lat],\n",
    "    datetime=\"2020-01-01/2020-12-31\",\n",
    "    query={\"eo:cloud_cover\": {\"gt\": 20}}  # asegurar nubes\n",
    ")\n",
    "\n",
    "\n",
    "items = list(search.get_items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33150981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import planetary_computer\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# 1. Corregimos el warning y configuramos el catálogo\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8d13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_pares_entrenamiento(bioma_nombre, coords, n_pares=50, dias_margen=30):\n",
    "    buffer = 0.1\n",
    "    bbox = [coords[0]-buffer, coords[1]-buffer, coords[0]+buffer, coords[1]+buffer]\n",
    "    \n",
    "    search_cloudy = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox,\n",
    "        datetime=\"2023-01-01/2023-12-31\",\n",
    "        query={\"eo:cloud_cover\": {\"gt\": 20, \"lt\": 60}}, \n",
    "        max_items=n_pares\n",
    "    )\n",
    "    cloudy_items = list(search_cloudy.items())\n",
    "\n",
    "    pares = []\n",
    "    for c_item in cloudy_items:\n",
    "        # Extraer fecha de la imagen nublada\n",
    "        fecha_nublada = c_item.datetime\n",
    "        fecha_inicio = (fecha_nublada - datetime.timedelta(days=dias_margen)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        fecha_fin = (fecha_nublada + datetime.timedelta(days=dias_margen)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "        # B. Buscamos la imagen limpia en ese rango específico de ±30 días\n",
    "        search_clear = catalog.search(\n",
    "            collections=[\"sentinel-2-l2a\"],\n",
    "            bbox=c_item.bbox,\n",
    "            datetime=f\"{fecha_inicio}/{fecha_fin}\",\n",
    "            query={\"eo:cloud_cover\": {\"lt\": 5}}, \n",
    "            max_items=1\n",
    "        )\n",
    "        clear_items = list(search_clear.items())\n",
    "        \n",
    "        if clear_items:\n",
    "            pares.append({\n",
    "                \"bioma\": bioma_nombre,\n",
    "                \"id_nublada\": c_item.id,\n",
    "                \"id_limpia\": clear_items[0].id,\n",
    "                \"nubes_porcentaje\": c_item.properties[\"eo:cloud_cover\"],\n",
    "                \"item_nublado\": c_item,\n",
    "                \"item_limpio\": clear_items[0]\n",
    "            })\n",
    "    return pares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando pares para bosque...\n",
      "Buscando pares para ciudad...\n",
      "Buscando pares para desierto...\n",
      "Buscando pares para tundra...\n",
      "Buscando pares para sabana...\n",
      "Buscando pares para manglar...\n"
     ]
    }
   ],
   "source": [
    "biomas = {\n",
    "    \"bosque\": [-84.0, 10.3],\n",
    "    \"ciudad\": [-74.0, 40.7],\n",
    "    \"desierto\": [25.0, 25.0],\n",
    "    \"tundra\": [68.0, 70.0],\n",
    "    \"sabana\": [34.0, -1.3],\n",
    "    \"manglar\": [-80.0, 0.5],\n",
    "    \"montaña\": [86.9, 27.9],\n",
    "    \"pradera\": [-100.0, 44.0],\n",
    "    \"humedal\": [-58.0, -34.5],\n",
    "    \"agricultura\": [-3.0, 40.0],\n",
    "    \"glaciar\": [-72.0, -50.0],\n",
    "    \"volcanico\": [-155.0, 19.4],\n",
    "    \"costa\": [-77.0, 24.5],\n",
    "    \"isla_tropical\": [-60.0, 14.0]\n",
    "}\n",
    "\n",
    "dataset_pares = []\n",
    "\n",
    "for nombre, coords in biomas.items():\n",
    "    print(f\"Buscando pares para {nombre}...\")\n",
    "    dataset_pares.extend(buscar_pares_entrenamiento(nombre, coords, n_pares=100))\n",
    "\n",
    "df_pares = pd.DataFrame(dataset_pares).drop(columns=['item_nublado', 'item_limpio'])\n",
    "print(f\"\\nTotal de pares encontrados: {len(df_pares)}\")\n",
    "display(df_pares.head())\n",
    "\n",
    "\n",
    "# Ejemplo: Analizar el primer par encontrado\n",
    "ejemplo = dataset_pares[0]['item_nublado']\n",
    "props = ejemplo.properties\n",
    "\n",
    "print(f\"Análisis del item: {ejemplo.id}\")\n",
    "print(f\"- Nubes totales: {props.get('s2:cloud_shadow_percentage')}%\")\n",
    "print(f\"- Nubes Cirrus (finas): {props.get('s2:thin_cirrus_percentage')}%\")\n",
    "print(f\"- Sombras: {props.get('s2:cloud_shadow_percentage')}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cb36ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL). Error loading \"C:\\Users\\Amalia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrioxarray\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL). Error loading \"C:\\Users\\Amalia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "class CloudSegmentationDataset(Dataset):\n",
    "    def __init__(self, df_pares, patch_size=256, train=True):\n",
    "        self.df = df_pares\n",
    "        self.patch_size = patch_size\n",
    "        self.train = train\n",
    "        self.bands = [\"B04\", \"B03\", \"B02\", \"B08\", \"B11\", \"B12\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def transform(self, image, mask, target):\n",
    "        # Image y Target son tensores (C, H, W). Mask es (1, H, W)\n",
    "        if self.train:\n",
    "            # Flip horizontal\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "                target = TF.hflip(target)\n",
    "\n",
    "            # Rotaciones de 90 grados\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            if angle != 0:\n",
    "                image = TF.rotate(image, angle)\n",
    "                mask = TF.rotate(mask, angle)\n",
    "                target = TF.rotate(target, angle)\n",
    "\n",
    "        return image, mask, target\n",
    "\n",
    "    def _get_patch(self, item, is_mask=False):\n",
    "        signed_item = planetary_computer.sign(item)\n",
    "        if is_mask:\n",
    "            url = signed_item.assets[\"SCL\"].href\n",
    "            da = rioxarray.open_rasterio(url)\n",
    "            patch = da.isel(y=slice(0, self.patch_size), x=slice(0, self.patch_size)).values\n",
    "            mask = np.isin(patch, [3, 8, 9, 10]).astype(np.float32)\n",
    "            return torch.from_numpy(mask) # Retorna (1, H, W) o (H, W)\n",
    "        else:\n",
    "            band_data = []\n",
    "            for b in self.bands:\n",
    "                url = signed_item.assets[b].href\n",
    "                da = rioxarray.open_rasterio(url)\n",
    "                patch = da.isel(y=slice(0, self.patch_size), x=slice(0, self.patch_size)).values\n",
    "                # Normalización robusta por banda: (x - min) / (max - min) aproximado\n",
    "                # Sentinel-2 escala 0-10000, pero raramente pasa de 4000 en reflectancia\n",
    "                patch = np.clip(patch / 4000.0, 0, 1)\n",
    "                band_data.append(patch)\n",
    "\n",
    "            img = np.concatenate(band_data, axis=0).astype(np.float32)\n",
    "            return torch.from_numpy(img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_input = self._get_patch(row['item_nublado'], is_mask=False)\n",
    "        mask = self._get_patch(row['item_nublado'], is_mask=True)\n",
    "        img_target = self._get_patch(row['item_limpio'], is_mask=False)\n",
    "\n",
    "        # Aplicar aumentos de datos de forma consistente a los 3 elementos\n",
    "        img_input, mask, img_target = self.transform(img_input, mask, img_target)\n",
    "\n",
    "        return {\n",
    "            \"input\": img_input,   # Para Modelo A y B\n",
    "            \"mask\": mask,         # Target para Modelo A, Input para Modelo B\n",
    "            \"target\": img_target  # Target para Modelo B\n",
    "        }\n",
    "\n",
    "# Crear el DataLoader\n",
    "train_ds = CloudSegmentationDataset(df_pares)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0287a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "def visualizar_comparativa(par, patch_size=512):\n",
    "    # 1. Firmar los items para obtener acceso a los datos\n",
    "    item_nublado = planetary_computer.sign(par['item_nublado'])\n",
    "    item_limpio = planetary_computer.sign(par['item_limpio'])\n",
    "    \n",
    "    # 2. Función unificada para extraer parches alineados\n",
    "    def get_data_aligned(item, is_scl=False):\n",
    "        bands = [\"B04\", \"B03\", \"B02\"] if not is_scl else [\"SCL\"]\n",
    "        data = []\n",
    "        \n",
    "        # Usamos la banda B04 (10m) como referencia espacial absoluta\n",
    "        with rioxarray.open_rasterio(item.assets[\"B04\"].href) as ref:\n",
    "            # Calculamos el centro basado en la resolución de 10m\n",
    "            cy, cx = ref.shape[1] // 2, ref.shape[2] // 2\n",
    "            # Definimos el área de recorte de referencia\n",
    "            ref_patch = ref.isel(y=slice(cy, cy+patch_size), x=slice(cx, cx+patch_size))\n",
    "\n",
    "        for b in bands:\n",
    "            with rioxarray.open_rasterio(item.assets[b].href) as da:\n",
    "                # Si es SCL (20m), re-muestreamos para que encaje en la rejilla de 10m\n",
    "                if is_scl:\n",
    "                    # Usamos Resampling.nearest (0) para no inventar clases intermedias\n",
    "                    da = da.rio.reproject_match(ref_patch, resampling=Resampling.nearest)\n",
    "                \n",
    "                # Extraemos el parche usando los mismos índices centrales\n",
    "                patch = da.isel(y=slice(cy, cy+patch_size), x=slice(cx, cx+patch_size)).values\n",
    "                data.append(patch)\n",
    "        \n",
    "        if is_scl:\n",
    "            # SCL devuelve (1, H, W), tomamos la primera capa\n",
    "            return data[0][0] \n",
    "        \n",
    "        # Para RGB: Stack canales y transponer a (H, W, C)\n",
    "        # Sentinel-2 se visualiza bien normalizando entre 0 y 3000-4000\n",
    "        rgb = np.concatenate(data, axis=0).transpose(1, 2, 0)\n",
    "        return np.clip(rgb / 3500, 0, 1)\n",
    "\n",
    "    # 3. Carga de datos procesados\n",
    "    print(f\"Procesando visualización para: {par['bioma']}...\")\n",
    "    img_nublada = get_data_aligned(item_nublado, is_scl=False)\n",
    "    img_limpia = get_data_aligned(item_limpio, is_scl=False)\n",
    "    mask_scl = get_data_aligned(item_nublado, is_scl=True)\n",
    "\n",
    "    # 4. Crear la visualización\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    ax[0].imshow(img_nublada)\n",
    "    ax[0].set_title(f\"Input: Nublado ({par['nubes_porcentaje']:.1f}%)\")\n",
    "    \n",
    "    ax[1].imshow(img_limpia)\n",
    "    ax[1].set_title(\"Target: Limpio (Ground Truth)\")\n",
    "    \n",
    "    # Visualización binaria de la máscara para el Modelo A\n",
    "    # Clases SCL: 3 (sombras), 8 (nubes media prob), 9 (alta prob), 10 (cirrus)\n",
    "    mask_binaria = np.isin(mask_scl, [3, 8, 9, 10])\n",
    "    ax[2].imshow(mask_binaria, cmap='gray')\n",
    "    ax[2].set_title(\"Máscara de Nubes/Sombras\")\n",
    "    \n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Ejecución\n",
    "visualizar_comparativa(dataset_pares[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica que cada fila tenga un ID distinto pero cercano\n",
    "print(df_pares[['id_nublada', 'id_limpia', 'nubes_porcentaje']].head(10))\n",
    "\n",
    "# Cuenta cuántos pares tienes por cada tipo de bioma\n",
    "print(df_pares['bioma'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
